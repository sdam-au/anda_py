
import json
import unicodedata
import re
import requests

### DATA (i.e. greek vocabulary)
try: # if possible, upload the data from the package
  morpheus_by_lemma = json.load(open("anda/data/morpheus_by_lemma.json"))
  morpheus_dict = json.load(open("anda/data/morpheus_dict.json"))
except: # if not, read them from sciencedata
  publicfolder = "8fe7d59de1eafe5f8eaebc0044534606"
  morpheus_by_lemma = json.loads(requests.get("https://sciencedata.dk/public/" + publicfolder + "/morpheus_by_lemma.json").content)
  morpheus_dict = json.loads(requests.get("https://sciencedata.dk/public/" + publicfolder + "/morpheus_dict.json").content)



### simple replacements
to_replace_dict={
    "á¼“":"á¼•",
    "á½¸" : "ÏŒ",
    "á½°" : "Î¬",
    "á½²" : "Î­",
    "á½¶" : "Î¯",
    "á½´" : "Î·",
    }

stopwords_string = "Î±á½Ï„á½¸Ï‚ Î±á½Ï„ÏŒÏ‚ Î³Îµ Î³á½°Ï Î³Î¬Ï Î´' Î´Î±á½¶ Î´Î±á½¶Ï‚ Î´Î±Î¯ Î´Î±Î¯Ï‚ Î´Î¹á½° Î´Î¹Î¬ Î´á½² Î´Î­ Î´á½´ Î´Î® Îµá¼° Îµá¼°Î¼á½¶ Îµá¼°Î¼Î¯ Îµá¼°Ï‚ Îµá¼´Î¼Î¹ ÎºÎ±Ï„á½° ÎºÎ±Ï„Î¬ ÎºÎ±á½¶ ÎºÎ±Î¯ Î¼ÎµÏ„á½° Î¼ÎµÏ„Î¬ Î¼á½²Î½ Î¼Î­Î½ Î¼á½´ Î¼Î® Î¿á¼± Î¿á½ Î¿á½Î´Îµá½¶Ï‚ Î¿á½Î´ÎµÎ¯Ï‚ Î¿á½Î´á½² Î¿á½Î´Î­ Î¿á½Îº Î¿á½”Ï„Îµ Î¿á½•Ï„Ï‰Ï‚ Î¿á½–Î½ Î¿á½—Ï„Î¿Ï‚ Ï€Î±Ïá½° Ï€Î±ÏÎ¬ Ï€ÎµÏá½¶ Ï€ÎµÏÎ¯ Ï€Ïá½¸Ï‚ Ï€ÏÏŒÏ‚ Ïƒá½¸Ï‚ ÏƒÏŒÏ‚ Ïƒá½º Ïƒá½ºÎ½ ÏƒÏ ÏƒÏÎ½ Ï„Îµ Ï„Î¹ Ï„Î¹Ï‚ Ï„Î¿Î¹Î¿á¿¦Ï„Î¿Ï‚ Ï„Î¿á½¶ Ï„Î¿Î¯ Ï„Î¿á½ºÏ‚ Ï„Î¿ÏÏ‚ Ï„Î¿á¿¦ Ï„á½° Ï„Î¬ Ï„á½´Î½ Ï„Î®Î½ Ï„á½¶ Ï„á½¶Ï‚ Ï„Î¯ Ï„Î¯Ï‚ Ï„á½¸ Ï„á½¸Î½ Ï„ÏŒ Ï„ÏŒÎ½ Ï„á¿†Ï‚ Ï„á¿‡ Ï„á¿¶Î½ Ï„á¿· á¼€Î»Î»' á¼€Î»Î»á½° á¼€Î»Î»Î¬ á¼€Ï€á½¸ á¼€Ï€ÏŒ á¼‚Î½ á¼„Î»Î»Î¿Ï‚ á¼„Î½ á¼„ÏÎ± á¼Î³á½¼ á¼Î³Ï á¼Îº á¼Î¾ á¼Î¼á½¸Ï‚ á¼Î¼ÏŒÏ‚ á¼Î½ á¼Ï€á½¶ á¼Ï€Î¯ á¼á½°Î½ á¼Î¬Î½ á¼‘Î±Ï…Ï„Î¿á¿¦ á¼”Ï„Î¹ á¼¡ á¼¢ á¼¤ á½ á½ƒÎ´Îµ á½ƒÏ‚ á½…Î´Îµ á½…Ï‚ á½…ÏƒÏ„Î¹Ï‚ á½…Ï„Î¹ á½‘Î¼á½¸Ï‚ á½‘Î¼ÏŒÏ‚ á½‘Ï€á½²Ï á½‘Ï€Î­Ï á½‘Ï€á½¸ á½‘Ï€ÏŒ á½¡Ï‚ á½¥ÏƒÏ„Îµ á½¦ Î¾ÏÎ½ Î¾á½ºÎ½ ÏƒÏÎ½ Ïƒá½ºÎ½ Ï„Î¿á¿–Ï‚ Ï„á¾¶Ï‚ Ï„Î·Î½ Î± Î¼Î·Î½ á¼ƒ ğ”š Î² Î´Î· Î´Î¹ Î´á¾¿ Î´Ê¼ Î´ Ï„ÏŒÏ„ á¼€Î»Î»Ê¼ á½…ÏƒÊ¼ á¼Ï€Ê¼ Î¹Î· â–³Î¶ Î¹Î² Ï„Ï‡ Î¼Î· ; á½ƒ Î³ . á½…Ï„Î±Î½ Ï€Î¿Ï„Î­ Î¿á½Î´Ê¼ ÎºÎ±Î¸Ê¼ á¼€Î»Î»á¾¿ Ï„Î·Î½ Î± Î¼Î·Î½ á¼ƒ ğ”š Î² Î´Î· Î´Î¹ Î´á¾¿ Î´Ê¼ Î´ Ï„ÏŒÏ„ á¼€Î»Î»Ê¼ á½…ÏƒÊ¼ á¼Ï€Ê¼ Î¹Î· â–³Î¶ Î¹Î² Ï„Ï‡ Î¼Î· ; á½ƒ Î³ á½…Ï„Î±Î½ Ï€Î¿Ï„Î­ Î¿á½Î´Ê¼ ÎºÎ±Î¸Ê¼ á¼€Î»Î»á¾¿ Ï„Î·Î½ Î± Î¼Î·Î½ á¼ƒ ğ”š Î² Î´Î· Î´Î¹ Î´á¾¿ Î´Ê¼ Î´ Ï„ÏŒÏ„ á¼€Î»Î»Ê¼ á½…ÏƒÊ¼ á¼Ï€Ê¼ Î¹Î· â–³Î¶ Î¹Î² Ï„Ï‡ Î¼Î· á½ƒ Î³ á½…Ï„Î±Î½ Ï€Î¿Ï„Î­ Î¿á½Î´Ê¼ ÎºÎ±Î¸Ê¼ á¼€Î»Î»á¾¿"
STOPS_LIST = stopwords_string.split()

def get_sentences(string):
  sentences = [s.strip() for s in re.split("\Â·|\.|\:|\;", unicodedata.normalize("NFC", string))]
  return sentences

def return_list_of_tokens(word, filter_by_postag=None, involve_unknown=False):
  word = unicodedata.normalize("NFC", word)
  try:
    list_of_tokens = morpheus_dict[word]
    if len(list_of_tokens) < 1:
      list_of_tokens = morpheus_dict[word.lower()]
      if len(list_of_tokens) < 1:
        list_of_tokens = [{"f":word, "i": "", "b":"", "l":word.lower(), "e":"", "p":"", "d":"", "s":"", "a":""}]
  except:
    list_of_tokens = [{"f":word, "i": "", "b":"", "l":word.lower(), "e":"", "p":"", "d":"", "s":"", "a":""}]

  if filter_by_postag != None:
    try:
      list_of_tokens_filtered = []
      for token in list_of_tokens:
        if token["p"][0] in filter_by_postag:
          list_of_tokens_filtered.append(token)
      list_of_tokens = list_of_tokens_filtered
    except:
      if involve_unknown == False:
        list_of_tokens = []
  return list_of_tokens

def return_all_unique_lemmata(word, filter_by_postag=None, involve_unknown=False):
  list_of_tokens = return_list_of_tokens(word, filter_by_postag=filter_by_postag, involve_unknown=involve_unknown)
  lemmata = "/".join(set([token["l"] for token in list_of_tokens]))
  return lemmata

def return_all_unique_translations(word, filter_by_postag=None, involve_unknown=False):
  list_of_tokens = return_list_of_tokens(word, filter_by_postag=filter_by_postag, involve_unknown=involve_unknown)
  try:
    translations = " / ".join(set([token["s"] for token in list_of_tokens]))
  except:
    translations = ""
  return translations

def return_first_lemma(word, filter_by_postag=None, involve_unknown=False):
  list_of_tokens = return_list_of_tokens(word, filter_by_postag=filter_by_postag, involve_unknown=involve_unknown)
  try:
    first_lemma = list_of_tokens[0]["l"]
  except:
    first_lemma = ""
  return first_lemma

def morphological_analysis(string):
  first_tokens = []
  string_tokenized = tokenize_string(string)
  string_analyzed = [return_list_of_tokens(word)[0] for word in string_tokenized if word != ""]
  return string_analyzed

def lemma_translator(word):
  try:
    translations = []
    for option in word.split("/"):
      translations.append(" / ".join(set([token["s"] for token in morpheus_by_lemma[option]])))
    translations = " / ".join(translations)
  except:
    translations = ""
  return translations

def tokenize_string(string):
  string = re.sub(r'[A-Za-z0-9]+', "", string)
  string = re.sub(r'[-,\(\)=\\\?Â·â€–\+;\.\:/\[\]\*â€”Â»Â«\Â§Ë˜â€â€\|]+', "", string)
  string = re.sub(r'[^\w\s]','', string)
  for k,v in to_replace_dict.items():
    string = string.replace(k,v)
  string = unicodedata.normalize("NFC", string)
  string_tokenized = string.split()
  string_tokenized = [word for word in string_tokenized if len(word) > 1]
  return string_tokenized

def lemmatize_string(string, all_lemmata=False, filter_by_postag=None, involve_unknown=False):
  string_tokenized = tokenize_string(string)
  string_tokenized = [word for word in string_tokenized if word not in STOPS_LIST]
  if all_lemmata==True:
    string_lemmatized = [return_all_unique_lemmata(word, filter_by_postag=filter_by_postag, involve_unknown=involve_unknown) for word in string_tokenized if word != ""]
  else: 
    string_lemmatized = [return_first_lemma(word, filter_by_postag=filter_by_postag, involve_unknown=involve_unknown) for word in string_tokenized if word != ""]  
  string_lemmatized = [word for word in string_lemmatized if word != ""]
  string_lemmatized = [re.sub(r'\d', "", w) for w in string_lemmatized if w not in STOPS_LIST]
  return string_lemmatized

def get_lemmatized_sentences(string, all_lemmata=False, filter_by_postag=None, involve_unknown=False):
  sentences = get_sentences(string)
  lemmatized_sentences = []
  for sentence in sentences:
    lemmatized_sentence = lemmatize_string(sentence, all_lemmata=all_lemmata, filter_by_postag=filter_by_postag, involve_unknown=involve_unknown)
    if len(lemmatized_sentence) > 0:
      lemmatized_sentences.append(lemmatized_sentence)
  return lemmatized_sentences